{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics Generation\n",
    "\n",
    "This notebook describes and implements the process of generating summary statistics and outputting them to a format that can be used by the companion notebook to generate synthetic data.  It makes use of the `behavioural_synthetic` library built by the Behavioural Insights Team.  \n",
    "\n",
    "### Requirements:\n",
    "- Python 3.11 or greater.\n",
    "    - Note that in the actual runs in the SRS an earlier version of Python (3.7) was installed and the library had to be adapted accordingly.  This should not make any difference on the level of this notebook, but be aware that it has not yet been tested with the more advanced version of the library and contains some functions (e.g. categorical censorship) that are not present in the 3.11 version.  Caveat emptor.\n",
    "\n",
    "### Setup: \n",
    "- Copy the following into your working directory:\n",
    "    - The `behavioural_synthetic` directory.\n",
    "    - The `requirements.txt` file.\n",
    "- Create a venv for Python using requirements.txt.\n",
    "- Set up the input and output directories.\n",
    "- Edit the notebook to use those input and output directories, as described later.\n",
    "- Run the relevant sections of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define libraries, shared functions and shared user-defined variables.\n",
    "\n",
    "This section describes library imports, useful functions and user-defined variables that are used throughout the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from math import ceil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from behavioral_synthetic.tables.columns.general_functions import read_data\n",
    "from behavioral_synthetic.tables.Table import Table\n",
    "from behavioral_synthetic.tables.test_Table import TestTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset properties\n",
    "\n",
    "The data was originally output in terms of a single, large .tsv file.  Here we define the names of data sets of interest in the form of a batch name (which sets a target directory for that batch) and a dictionary `datasets`.  `datasets` is of the form `{ ${DATASET_NAME}: {\"ID\": ${PROJECT_ID}},...}` where `${DATASET_NAME}` is the name of the dataset and `${PROJECT_ID}` identifies the dataset within the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_name =\"BATCH13\" #defines target directory for batch\n",
    "\n",
    "datasets = {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared function\n",
    "\n",
    "This locates whether a given file is present in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_in_directory(directory, filename):\n",
    "    file_list = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    return filename in file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract datasets for individual projects from master file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code finds the data sets of interest in the combined file of all projects and, if they have not already been output as a distinct .tsv file, does so.  It also ensures (using the `data_types` and `nulls`) that a list of ID variables are set to have null values, so that the final steps of synthetic data generation (in the companion notebook) can provide suitable customised values. \n",
    "\n",
    "- `source_file` is the path to the combined file\n",
    "- `output_directory` is the path to the output directory where the individual .tsv files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_list = [set for set in datasets]\n",
    "\n",
    "source_file = \"\"\n",
    "output_directory = \"\"\n",
    "\n",
    "old_file_list = listdir(output_directory)\n",
    "source_data_loaded = False\n",
    "\n",
    "data_types = {\n",
    "    \"Anon_School_ID\": \"object\",\n",
    "    \"Anon_Class_ID\": \"object\",\n",
    "    \"Anon_Teacher_ID\": \"object\",\n",
    "    \"Anon_Pupil_ID\": \"object\"\n",
    "}\n",
    "\n",
    "nulls = {\n",
    "    \"Anon_School_ID\": \"\",\n",
    "    \"Anon_Class_ID\": \"\",\n",
    "    \"Anon_Teacher_ID\": \"\",\n",
    "    \"Anon_Pupil_ID\": \"\"\n",
    "}\n",
    "\n",
    "for data_set in set_list:\n",
    "    output_file = f\"{data_set}_original_data.tsv\"\n",
    "    dataset = datasets[data_set][\"ID\"]\n",
    "    file_in_directory = is_file_in_directory(output_directory, output_file)\n",
    "    \n",
    "    if (not file_in_directory) and (not source_data_loaded):\n",
    "        source_data = pd.read_csv(source_file, sep='\\t', encoding='cp1252', low_memory=False, dtype=data_types, na_values=nulls)\n",
    "        source_data_loaded = True\n",
    "        \n",
    "    if source_data_loaded and (not file_in_directory):\n",
    "        print(f\"Generating {output_file}.\")\n",
    "        project_data = source_data[source_data['Project']==dataset]\n",
    "        project_data.to_csv(f\"{output_directory}\\\\{output_file}\", index=False, sep='\\t')\n",
    "    else:\n",
    "        print (f\"File {output_file} has already been generated.\")\n",
    "\n",
    "if source_data_loaded:\n",
    "    del source_data\n",
    "gc.collect()\n",
    "\n",
    "new_files = [f for f in listdir(output_directory) if f not in old_file_list]\n",
    "if len(new_files) > 0:\n",
    "    new_file_list = ',\\n'.join(new_files)\n",
    "    print(\"\\nThe following new input files have been generated:\\n\"+f\"{new_file_list}.\")\n",
    "else:\n",
    "    print(\"\\nNo new input files generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate summary statistics from individual project data sets\n",
    "\n",
    "This portion of the code generates summary statistics for the data from the .tsv files.\n",
    "\n",
    "- You will need to set `original_data_file` to contain the path to the directory in which the .tsv files are stored and to have the correct name pattern of those files.\n",
    "- You will also need to set `output_data_directory` to the path to the directory in which the summary statistics in json format will be stored.\n",
    "- If `regenerate=True`, already existing output files will be overwritten, otherwise they will not.\n",
    "- If `read_in_columns=True`, you will need to set `column_data_directory` to the path of a manually defined set of column definitions.  Otherwise a set of heuristics will be used to define the type of each column. The code in the Appendix can generate a column definition file from summary statistics output.\n",
    "- Note that the automatic censoring of low-count categorical values has only been implemented in the SRS version of the synthetic data generation code. *If you use the non-SRS version, you should check that you are in compliance with any such disclosure requirements before requesting data release, and/or update the code with the relevant portion of the code from `summary_data/behavioural_synthetic_SRS/tables/columns/CategoricalVersion.py`.*\n",
    "\n",
    "In most cases, this is the final step of the process.  However, in cases where a more legible format (including counts) is required for this output, the next two sections may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_list = [set for set in datasets]\n",
    "regenerate = False  #Do not overwrite/regenerate existing results\n",
    "read_in_columns = False #Perform heuristic analysis of columns\n",
    "#regenerate = True   #Overwrite/regenerate existing results -- careful with this value!\n",
    "#read_in_columns = True #Read in column values from an external file\n",
    "\n",
    "output_data_directory = f\"\"\n",
    "old_file_list = listdir(output_data_directory)\n",
    "\n",
    "if read_in_columns:\n",
    "    column_data_directory = \"\"\n",
    "\n",
    "\n",
    "for data_set in set_list:\n",
    "    original_data_file = f\"\"\n",
    "    output_data_file = f\"{data_set}_summary_stats.json\"\n",
    "    file_in_directory = is_file_in_directory(output_data_directory, output_data_file)\n",
    "\n",
    "    if regenerate or (not file_in_directory):\n",
    "        if regenerate and file_in_directory:\n",
    "            print(f\"Regenerating {output_data_file}.\")\n",
    "            old_file_list.remove(output_data_file)  # want the summary of new files to count regenerated ones\n",
    "        else:\n",
    "            print(f\"Generating {output_data_file}.\")\n",
    "            \n",
    "        original_data = pd.read_csv(original_data_file, sep='\\t')\n",
    "        original_table = Table(table=original_data, table_name = data_set)\n",
    "        \n",
    "        if read_in_columns:\n",
    "            print(f\"Using user defined columns\")\n",
    "            with open(f\"{column_data_directory}\\\\{data_set}_column_types_checked.json\", 'r') as file:\n",
    "                column_data = json.load(file)\n",
    "            original_table.analyse_with_column_list(columns_list=column_data)\n",
    "        else:\n",
    "            print(f\"Using heuristic analysis of the columns\")\n",
    "            original_table.analyse(decimal_accuracy = 7)\n",
    "\n",
    "        table_statistics = original_table.dictionary_out()\n",
    "        #print(table_statistics)\n",
    "        \n",
    "\n",
    "        with open(f\"{output_data_directory}\\\\{output_data_file}\", 'w') as file:\n",
    "            json.dump(table_statistics, file, indent=4)\n",
    "    else:\n",
    "        print(f\"{output_data_file} already exists and regenerate is set to False.\")\n",
    "\n",
    "new_files = [f for f in listdir(output_data_directory) if f not in old_file_list]\n",
    "if len(new_files) > 0:\n",
    "    new_file_list = ',\\n'.join(new_files)\n",
    "    print(\"\\nThe following new summary statistics files have been (re)generated:\\n\"+f\"{new_file_list}.\")\n",
    "else:\n",
    "    print(\"\\nNo new summary statistics files (re)generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating formatted output files for SRS checking\n",
    "\n",
    "The SRS required legible files in a non-json format alongside the counts used to generate data so that they could be easily checked before release.  For actual exported output, we required the suppression of counts as they're potentially revealing.  Code for both is provided below.\n",
    "\n",
    "NOTE: While the code for generating files without counts doesn't currently do so, we also recommend removing any values with zero frequency (i.e. censored values) from the data you wish to export, in order to further reduce the risk of accidental disclosure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases you will need to set `output_data_directory` to the desired location of the output files, `summary_stats_file` to the path and filename pattern of the tsv files and output `output_data_file` to the desired filename pattern of the output. If `regenerate=True`, already existing output files will be overwritten, otherwise they will not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for conversion into .json input is provided in `generate_data/utilities/`, consisting of `convert_from_srs.py` and `test_convert.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatted output file with counts (for checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the formatted output files\n",
    "\n",
    "ID_COLUMNS = [\n",
    "    \"Project_Row_ID\",\n",
    "    \"Anon_School_ID\",\n",
    "    \"Anon_Class_ID\",\n",
    "    \"Anon_Teacher_ID\",\n",
    "    \"Anon_Pupil_ID\"\n",
    "]\n",
    "\n",
    "def format_column(column, number_of_rows):\n",
    "    \n",
    "    def is_iD(column):\n",
    "        return (column[\"Name\"] in ID_COLUMNS) and (column[\"Type\"] != \"empty\")\n",
    "    \n",
    "    is_id_col= is_iD(column)\n",
    "    \n",
    "    if is_id_col:\n",
    "        column[\"Type\"] == \"text\"\n",
    "    \n",
    "    if column[\"Type\"] == \"text\" or is_id_col: #is_ID(column):\n",
    "        return f\"\"\"\n",
    "# BEGIN COLUMN\n",
    "    COLUMN NAME: {column[\"Name\"]}\n",
    "    COLUMN TYPE: \"text\"\n",
    "# END COLUMN\n",
    "\n",
    "\"\"\"\n",
    "    elif column[\"Type\"] == \"empty\":\n",
    "        return f\"\"\"\n",
    "# BEGIN COLUMN\n",
    "    COLUMN NAME: {column[\"Name\"]}\n",
    "    COLUMN TYPE: {column[\"Type\"]}\n",
    "    \n",
    "    COLUMN VALUE: {column[\"all_values\"]}   COUNT: {number_of_rows}\n",
    "\n",
    "# END COLUMN\n",
    "\n",
    "\"\"\"\n",
    "    elif column[\"Type\"]==\"numeric\":\n",
    "        return f\"\"\"\n",
    "#BEGIN COLUMN\n",
    "    COLUMN NAME: {column[\"Name\"]}\n",
    "    COLUMN TYPE: {column[\"Type\"]}\n",
    "\n",
    "    DECIMAL PRECISION: {column[\"decimal_precision\"]}\n",
    "    IS INTEGER: {column[\"is_integer\"]}\n",
    "    \n",
    "    MEAN: {column[\"mean\"]}  STANDARD DEVIATION: {column[\"standard_deviation\"]}  COUNT {column[\"count for mean/standard deviation\"]}\n",
    "    \n",
    "    AVERAGED MAXIMUM AND MINIMUM {column[\"averaged_max_and_min\"]}\n",
    "    MAXIMUM: {column[\"maximum\"]}    COUNT: {column[\"# of values in average_max_min\"]}\n",
    "    MINIMUM: {column[\"minimum\"]}    COUNT: {column[\"# of values in average_max_min\"]}\n",
    "    \n",
    "    MISSING VALUES FREQUENCY: {column[\"missing_value_freq\"]}  COUNT: {column[\"count for missing\"]}\n",
    "    \n",
    "    \n",
    "#END COLUMN\n",
    "    \n",
    "\"\"\"\n",
    "    elif column[\"Type\"] == \"categorical\":\n",
    "        header = f\"\"\"\n",
    "#BEGIN COLUMN\n",
    "    COLUMN NAME: {column[\"Name\"]}\n",
    "    COLUMN TYPE: {column[\"Type\"]}\n",
    "    \n",
    "    FREQUENCIES:\n",
    "\"\"\"\n",
    "        footer = f\"\"\"\n",
    "        \n",
    "    NOTE: {column[\"Note\"]}   \n",
    "    \n",
    "#END COLUMN\n",
    "\n",
    "\"\"\"\n",
    "        key_list = [key for key in column if key not in [\"Name\", \"Type\", \"Disclosure\", \"Note\", \"Counts\" ]]\n",
    "        output = [header]\n",
    "        for key in key_list:\n",
    "            format_text = f\"\"\"    VALUE: {key}        FREQUENCY: {column[key]}   COUNT: {column[\"Counts\"][key]}\"\"\"\n",
    "            output.append(format_text)\n",
    "            \n",
    "        output.append(footer)\n",
    "        return '\\n'.join(output)\n",
    "    else:\n",
    "        print (\"Format type undefined\")\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "    \n",
    "\n",
    "set_list = [set for set in datasets]\n",
    "regenerate = False\n",
    "#regenerate = True   #Overwrite/regenerate existing results -- careful with this value!\n",
    "\n",
    "\n",
    "output_data_directory = f\"\"\n",
    "old_file_list = listdir(output_data_directory)\n",
    "\n",
    "for data_set in set_list:\n",
    "    summary_stats_file = f\"\"\n",
    "    output_data_file = f\"\"\n",
    "    file_in_directory = is_file_in_directory(output_data_directory, output_data_file)\n",
    "    \n",
    "    if regenerate or (not file_in_directory):\n",
    "        if regenerate and file_in_directory:\n",
    "            print(f\"Regenerating {output_data_file}.\")\n",
    "            old_file_list.remove(output_data_file)  # want the summary of new files to count regenerated ones\n",
    "        else:\n",
    "            print(f\"Generating {output_data_file}.\")\n",
    "\n",
    "        with open(summary_stats_file, 'r') as file:\n",
    "            summary_stats = json.load(file)\n",
    "        \n",
    "        column_data = summary_stats[\"Column_details\"]\n",
    "        \n",
    "        # rounds to the next highest hundred to prevent secondary disclosure\n",
    "        summary_stats[\"Number_of_rows\"] = ceil(summary_stats[\"Number_of_rows\"]*0.01)*100 \n",
    "       \n",
    "        \n",
    "        start = f\"\"\"\n",
    "#BEGIN TABLE SUMMARY\n",
    "    TABLE NAME: {summary_stats[\"Table_name\"]}\n",
    "    TABLE TYPE: {summary_stats[\"Table_type\"]}\n",
    "    NUMBER OF ROWS: {summary_stats[\"Number_of_rows\"]}\n",
    "#END TABLE SUMMARY    \n",
    "        \n",
    "        \"\"\"\n",
    "        output = [start]\n",
    "        \n",
    "        for column in column_data:\n",
    "            output.append(format_column(column, summary_stats[\"Number_of_rows\"]))\n",
    "            \n",
    "        final_output = \"\\n\".join(output)\n",
    "        \n",
    "        with open(f\"{output_data_directory}\\\\{output_data_file}\", \"w\") as file:\n",
    "            file.write(final_output)\n",
    "    \n",
    "               \n",
    "    else:\n",
    "        print(f\"{output_data_file} already exists and regenerate is set to False.\")\n",
    "        \n",
    "        \n",
    "new_files = [f for f in listdir(output_data_directory) if f not in old_file_list]\n",
    "if len(new_files) > 0:\n",
    "    new_file_list = ',\\n'.join(new_files)\n",
    "    print(\"\\nThe following formatted summary statistics files have been (re)generated:\\n\"+f\"{new_file_list}.\")\n",
    "else:\n",
    "    print(\"\\nNo new formatted summary statistics files (re)generated.\")\n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatted output files without counts (for export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the formatted output files without counts\n",
    "ID_COLUMNS = [\n",
    "    \"Project_Row_ID\",\n",
    "    \"Anon_School_ID\",\n",
    "    \"Anon_Class_ID\",\n",
    "    \"Anon_Teacher_ID\",\n",
    "    \"Anon_Pupil_ID\"\n",
    "]\n",
    "\n",
    "def format_column(column, number_of_rows):\n",
    "    \n",
    "    def is_iD(column):\n",
    "        return (column[\"Name\"] in ID_COLUMNS) and (column[\"Type\"] != \"empty\")\n",
    "    \n",
    "    is_id_col= is_iD(column)\n",
    "    \n",
    "    if is_id_col:\n",
    "        column[\"Type\"] == \"text\"\n",
    "    \n",
    "    if column[\"Type\"] == \"text\" or is_id_col: \n",
    "        return f\"\"\"\n",
    "# BEGIN COLUMN\n",
    "    COLUMN NAME: {column[\"Name\"]}\n",
    "    COLUMN TYPE: \"text\"\n",
    "# END COLUMN\n",
    "\n",
    "\"\"\"\n",
    "    elif column[\"Type\"] == \"empty\":\n",
    "        return f\"\"\"\n",
    "# BEGIN COLUMN\n",
    "    COLUMN NAME: {column[\"Name\"]}\n",
    "    COLUMN TYPE: {column[\"Type\"]}\n",
    "    \n",
    "    COLUMN VALUE: {column[\"all_values\"]}   \n",
    "\n",
    "# END COLUMN\n",
    "\n",
    "\"\"\"\n",
    "    elif column[\"Type\"]==\"numeric\":\n",
    "        return f\"\"\"\n",
    "#BEGIN COLUMN\n",
    "    COLUMN NAME: {column[\"Name\"]}\n",
    "    COLUMN TYPE: {column[\"Type\"]}\n",
    "\n",
    "    DECIMAL PRECISION: {column[\"decimal_precision\"]}\n",
    "    IS INTEGER: {column[\"is_integer\"]}\n",
    "    \n",
    "    MEAN: {column[\"mean\"]}  STANDARD DEVIATION: {column[\"standard_deviation\"]}  \n",
    "    \n",
    "    AVERAGED MAXIMUM AND MINIMUM {column[\"averaged_max_and_min\"]}\n",
    "    MAXIMUM: {column[\"maximum\"]}    \n",
    "    MINIMUM: {column[\"minimum\"]}    \n",
    "    \n",
    "    MISSING VALUES FREQUENCY: {column[\"missing_value_freq\"]}  \n",
    "    \n",
    "    \n",
    "#END COLUMN\n",
    "    \n",
    "\"\"\"\n",
    "    elif column[\"Type\"] == \"categorical\":\n",
    "        header = f\"\"\"\n",
    "#BEGIN COLUMN\n",
    "    COLUMN NAME: {column[\"Name\"]}\n",
    "    COLUMN TYPE: {column[\"Type\"]}\n",
    "    \n",
    "    FREQUENCIES:\n",
    "\"\"\"\n",
    "        footer = f\"\"\"\n",
    "        \n",
    "    NOTE: {column[\"Note\"]}   \n",
    "    \n",
    "#END COLUMN\n",
    "\n",
    "\"\"\"\n",
    "        key_list = [key for key in column if key not in [\"Name\", \"Type\", \"Disclosure\", \"Note\", \"Counts\" ]]\n",
    "        output = [header]\n",
    "        for key in key_list:\n",
    "            format_text = f\"\"\"    VALUE: {key}        FREQUENCY: {column[key]}   \"\"\"\n",
    "            output.append(format_text)\n",
    "            \n",
    "        output.append(footer)\n",
    "        return '\\n'.join(output)\n",
    "    else:\n",
    "        print (\"Format type undefined\")\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "    \n",
    "\n",
    "set_list = [set for set in datasets]\n",
    "regenerate = False\n",
    "#regenerate = True   #Overwrite/regenerate existing results -- careful with this value!\n",
    "\n",
    "\n",
    "output_data_directory = f\"\"\n",
    "old_file_list = listdir(output_data_directory)\n",
    "\n",
    "for data_set in set_list:\n",
    "    summary_stats_file = f\"\"\n",
    "    output_data_file = f\"{data_set}_summary_stats_formatted.txt\"\n",
    "    file_in_directory = is_file_in_directory(output_data_directory, output_data_file)\n",
    "    \n",
    "    if regenerate or (not file_in_directory):\n",
    "        if regenerate and file_in_directory:\n",
    "            print(f\"Regenerating {output_data_file}.\")\n",
    "            old_file_list.remove(output_data_file)  # want the summary of new files to count regenerated ones\n",
    "        else:\n",
    "            print(f\"Generating {output_data_file}.\")\n",
    "\n",
    "        with open(summary_stats_file, 'r') as file:\n",
    "            summary_stats = json.load(file)\n",
    "        \n",
    "        column_data = summary_stats[\"Column_details\"]\n",
    "        \n",
    "        # rounds to the next highest hundred to prevent secondary disclosure\n",
    "        summary_stats[\"Number_of_rows\"] = ceil(summary_stats[\"Number_of_rows\"]*0.01)*100 \n",
    "       \n",
    "        \n",
    "        start = f\"\"\"\n",
    "#BEGIN TABLE SUMMARY\n",
    "    TABLE NAME: {summary_stats[\"Table_name\"]}\n",
    "    TABLE TYPE: {summary_stats[\"Table_type\"]}\n",
    "    NUMBER OF ROWS: {summary_stats[\"Number_of_rows\"]}\n",
    "#END TABLE SUMMARY    \n",
    "        \n",
    "        \"\"\"\n",
    "        output = [start]\n",
    "        \n",
    "        for column in column_data:\n",
    "            output.append(format_column(column, summary_stats[\"Number_of_rows\"]))\n",
    "            \n",
    "        final_output = \"\\n\".join(output)\n",
    "        \n",
    "        with open(f\"{output_data_directory}\\\\{output_data_file}\", \"w\") as file:\n",
    "            file.write(final_output)\n",
    "    \n",
    "               \n",
    "    else:\n",
    "        print(f\"{output_data_file} already exists and regenerate is set to False.\")\n",
    "        \n",
    "        \n",
    "new_files = [f for f in listdir(output_data_directory) if f not in old_file_list]\n",
    "if len(new_files) > 0:\n",
    "    new_file_list = ',\\n'.join(new_files)\n",
    "    print(\"\\nThe following formatted summary statistics files have been (re)generated:\\n\"+f\"{new_file_list}.\")\n",
    "else:\n",
    "    print(\"\\nNo new formatted summary statistics files (re)generated.\")\n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Generate a description file for columns from the output file for later editing\n",
    "\n",
    "If you wish to modify the classification of different columns, this will generate files from the json summary statistics output that can be edited and read in to change the type of a given column.\n",
    "\n",
    "You will need to set `output_data_directory` to the desired location of the output files, `summary_stats_file` to the path and filename pattern of the tsv files and output `output_data_file` to the desired filename pattern of the output. If `regenerate=True`, already existing output files will be overwritten, otherwise they will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_from_output_dict(output_dict):\n",
    "    \n",
    "    if output_dict['Type'] == 'numeric':\n",
    "        fields = ['Name','Type', 'decimal_precision', 'averaged_max_and_min']\n",
    "    elif output_dict['Type'] in ['date', 'time', 'datetime']:\n",
    "        fields = ['Name','Type', 'format', 'averaged_max_and_min']\n",
    "    else:\n",
    "        fields = ['Name', 'Type']\n",
    "        \n",
    "    return {field: output_dict[field] for field in fields}\n",
    "\n",
    "\n",
    "set_list = [set for set in datasets]\n",
    "regenerate = False  #Do not overwrite/regenerate existing results\n",
    "regenerate = True   #Overwrite/regenerate existing results -- careful with this value!\n",
    "\n",
    "output_data_directory = f\"\"\n",
    "old_file_list = listdir(output_data_directory)\n",
    "\n",
    "for data_set in set_list:\n",
    "    summary_stats_file = f\"\"\n",
    "    output_data_file = f\"{data_set}_column_types.json\"\n",
    "    file_in_directory = is_file_in_directory(output_data_directory, output_data_file)\n",
    "\n",
    "    if regenerate or (not file_in_directory):\n",
    "        if regenerate and file_in_directory:\n",
    "            print(f\"Regenerating {output_data_file}.\")\n",
    "            old_file_list.remove(output_data_file)  # want the summary of new files to count regenerated ones\n",
    "        else:\n",
    "            print(f\"Generating {output_data_file}.\")\n",
    "\n",
    "        with open(summary_stats_file, 'r') as file:\n",
    "            summary_stats = json.load(file)\n",
    "        \n",
    "        column_data = [columns_from_output_dict(column) for column in summary_stats[\"Column_details\"]]\n",
    "        \n",
    "    \n",
    "        with open(f'{output_data_directory}//{output_data_file}', 'w') as file:\n",
    "            json.dump(column_data, file, indent = 4)\n",
    "            \n",
    "    else:\n",
    "        print(f\"{output_data_file} already exists and regenerate is set to False.\")\n",
    "        \n",
    "        \n",
    "new_files = [f for f in listdir(output_data_directory) if f not in old_file_list]\n",
    "if len(new_files) > 0:\n",
    "    new_file_list = ',\\n'.join(new_files)\n",
    "    print(\"\\nThe following new columns description files have been (re)generated:\\n\"+f\"{new_file_list}.\")\n",
    "else:\n",
    "    print(\"\\nNo new summary statistics files (re)generated.\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
